"""
SemanticTagger.py

è¯­ä¹‰æ ‡ç­¾å™¨

ç”Ÿæˆæœ€æœ‰æ•ˆçš„è¯­ä¹‰æ ‡ç­¾ã€‚

åˆ›å»ºæ—¥æœŸï¼š2025-04-27
"""
import os
import ast
import json
import random
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional, TypeVar
from unittest.mock import MagicMock, patch

from pymilvus import MilvusClient
from utils import get_embedding, tag_records, tag_contents

class SemanticTagger:
    def __init__(self, tags_design_path: str = "", batch_size: int = 5):
        self.tags_design_path = tags_design_path
        if self.tags_design_path != "":
            self._load_tags_design()
        else:
            self.tags_design = {}
        self.batch_size = batch_size
        load_dotenv()  # åŠ è½½ç¯å¢ƒå˜é‡ï¼ŒåŒ…æ‹¬API Keyå’Œbase_url
        milvus_uri = os.getenv("MILVUS_URI", "http://localhost:19530")
        milvus_token = os.getenv("MILVUS_TOKEN", "root:Milvus")
        self.client = MilvusClient(uri = milvus_uri, token=milvus_token)
        self.collection_name = os.getenv("MILVUS_COLLECTION", "KnowledgeBaseRecords")


    def InsertTimeTagging(
        self,
        records: List[Dict],
        batch_size: int = 10,
        sample_scale: int = 20,
        update_reference_records: bool = False
    ) -> List[Dict]:
        """
        æ’å…¥æ—¶æ ‡è®°ï¼šåŸºäºç›¸ä¼¼æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼Œä¸ºæ¯æ¡è®°å½•ç”Ÿæˆæ›´å…·åŒºåˆ†åº¦çš„è¯­ä¹‰æ ‡ç­¾ã€‚
        å¯é€šè¿‡å‚æ•° `update_reference_records` æ§åˆ¶æ˜¯å¦å¯¹å‚è€ƒæ–‡æ¡£è¿›è¡Œæ›´æ–°ã€‚

        Args:
            records (List[Record]): å¾…æ‰“æ ‡çš„è®°å½•åˆ—è¡¨ï¼›
            batch_size (int): æ„é€ å‚è€ƒæ–‡æ¡£é›†çš„å¤§å°ï¼›
            sample_scale (int): ä» Milvus ä¸­å¬å›çš„ç›¸ä¼¼æ–‡æ¡£æ•°é‡ï¼›
            update_reference_records (bool): æ˜¯å¦æ›´æ–°å‚ä¸æ‰¹æ¬¡çš„ reference_recordsï¼›

        Returns:
            List[Record]: å®é™…æ›´æ–°äº†æ ‡ç­¾çš„è®°å½•åˆ—è¡¨ï¼ˆå« current_record å’Œ changed reference_recordsï¼‰
        """
        updated_records = []

        for record in records:
            # Step 1: åŸºäºå½“å‰è®°å½•å†…å®¹ï¼Œå¬å› top_k ç›¸ä¼¼æ–‡æ¡£ï¼Œæ„å»ºå‚è€ƒæ–‡æ¡£é›†
            query_embedding = get_embedding(record.get("content"))
            hits = self.client.search(
                collection_name=self.collection_name,
                data=[query_embedding],
                anns_field="embedding",
                # param={"metric_type": "COSINE", "nprobe": 10},
                limit=sample_scale,
                output_fields=["id", "content", "tags", "embedding"]
            )
            batch_record_dict = []
            for hit in hits[0]:  # hits æ˜¯åµŒå¥—åˆ—è¡¨ï¼Œæ¯ä¸ªæŸ¥è¯¢ç»“æœå¯¹åº”ä¸€ä¸ªå­åˆ—è¡¨
                entity = hit.get("entity", {})
                tags_str = entity.get("tags", "[]")  # é»˜è®¤ç©ºåˆ—è¡¨
                try:
                    tags_list = ast.literal_eval(tags_str)
                except (ValueError, SyntaxError):
                    tags_list = []  # å¦‚æœè§£æå¤±è´¥ï¼Œå°±è®¾ä¸ºç©ºåˆ—è¡¨

                batch_record_dict.append({
                    "id": hit.get("id"),
                    "content": entity.get("content", ""),
                    "tags": tags_list,  # ç¡®ä¿ä¼ å…¥çš„æ˜¯åˆ—è¡¨
                })
            # Step 2: éšæœºé‡‡æ ·
            target_record_dict = batch_record_dict[0]

            random.sample(batch_record_dict[1:-1], k=batch_size-1)  # éšæœºé‡‡æ ·

            contents = [target_record_dict.get("content")]
            contents.extend([record_dict.get("content") for record_dict in batch_record_dict])
            # Step 3: æ‰¹é‡ç”Ÿæˆæ ‡ç­¾
            batch_tags = tag_contents(contents = contents)

            # Step 4: æ›´æ–° batch_record_dict ä¸­çš„æ ‡ç­¾ï¼Œå¹¶å†™å…¥æ•°æ®åº“
            for i, tags in enumerate(batch_tags):
                former_tags = batch_record_dict[i]["tags"]
                if former_tags != tags:  # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦æœ‰å˜åŒ–
                    final_tags = list(set(former_tags + tags))  # åˆå¹¶æ ‡ç­¾å¹¶å»é‡
                    self.client.upsert(  # æ›´æ–° Milvus ä¸­çš„æ ‡ç­¾
                        collection_name=self.collection_name,
                        data=[{
                            "id": batch_record_dict[i]["id"],
                            "tags": final_tags,  # ç¡®ä¿ä¼ å…¥çš„æ˜¯åˆ—è¡¨
                        }])
                    updated_records.append({  # è®°å½•æ›´æ–°çš„è®°å½•
                        "id": batch_record_dict[i]["id"],
                        "contents": batch_record_dict[i]["content"],
                        "tags": final_tags,
                    })
                # å¦‚æœä¸æ›´æ–°å‚è€ƒè®°å½•ï¼Œåˆ™åœ¨å¤„ç†ç¬¬ä¸€æ¡è®°å½•åé€€å‡ºå¾ªç¯ã€‚
                if not update_reference_records:  # æ›´æ–° reference_records
                    break

        return updated_records

    def RetrievalTimeTagging(self, records: List[Dict] = None, batch_size: int = 3) -> List[Dict]:
        """
        å¬å›æ—¶æ ‡è®°ï¼šå¯¹ç»™å®šè®°å½•å­—å…¸è¿›è¡Œè¯­ä¹‰æ ‡ç­¾è¡¥å……ã€‚

        Args:
            records (List[Dict]): å¾…æ‰“æ ‡çš„è®°å½•åˆ—è¡¨ã€‚é¢„è®¾å·²ç»å…·å¤‡id, content, tagså’Œembeddingå­—æ®µã€‚
            batch_size (int): æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤ä¸º self.batch_size

        Returns:
            List[Dict]: æ›´æ–°æ ‡ç­¾åçš„è®°å½•åˆ—è¡¨
        """
        if not records:
            return records

        print(f"å…±æœ‰{len(records)}æ¡è®°å½•éœ€è¦æ‰“æ ‡ç­¾")

        batch_size = batch_size or self.batch_size

        batches = []
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            batches.append(batch)

        for batch in batches:
            # print(f"æ­£åœ¨å¤„ç†ç¬¬{batches.index(batch) + 1}æ‰¹")
            batch_contents = [record.get("content") for record in batch]
            batch_tags = [record.get("tags", []) for record in batch]
            new_tags = tag_contents(batch_contents)
            for i, tags in enumerate(new_tags):
                final_tags = list(set(batch_tags[i] + tags))
                batch[i]["tags"] = final_tags

        return records





    def _load_tags_design(self) -> Dict[str, Any]:
        """
        åŠ è½½æ ‡ç­¾è®¾è®¡æ–‡ä»¶ã€‚
        """
        if not os.path.exists(self.tags_design_path):
            raise FileNotFoundError(f"æ ‡ç­¾è®¾è®¡æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{self.tags_design_path}")
        with open(self.tags_design_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
        return data


def test_InsertTimeTagging():
    """
    æµ‹è¯• InsertTimeTagging æ–¹æ³•ï¼š
    - èƒ½å¦æ­£ç¡®ä» Milvus è·å–ç›¸ä¼¼è®°å½•ï¼›
    - èƒ½å¦è°ƒç”¨ tag_contents ç”Ÿæˆæ ‡ç­¾ï¼›
    - èƒ½å¦æ›´æ–°æ•°æ®åº“ä¸­çš„æ ‡ç­¾ã€‚
    """
    print("\n=== å¼€å§‹æµ‹è¯• InsertTimeTagging ===")

    # åˆå§‹åŒ– SemanticTagger å®ä¾‹
    tagger = SemanticTagger(batch_size=2)

    # Mock Milvus client
    mock_client = MagicMock()
    tagger.client = mock_client

    # æ„é€ æµ‹è¯•è¾“å…¥è®°å½•
    test_record = {
        "id": 999,
        "content": "æ·±åº¦å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯"
    }

    # æ„å»º Milvus è¿”å›çš„ hits æ•°æ®ç»“æ„
    mock_hits = [[{
        "id": 100,
        "entity": {
            "id": 100,
            "content": "æœºå™¨å­¦ä¹ åœ¨é‡‘èä¸­çš„åº”ç”¨",
            "tags": "['AI', 'æ•°æ®ç§‘å­¦']",
            "embedding": [0.1, 0.2, 0.3]
        }
    }, {
        "id": 101,
        "entity": {
            "id": 101,
            "content": "ç¥ç»ç½‘ç»œä¸å›¾åƒè¯†åˆ«",
            "tags": "['è®¡ç®—æœºè§†è§‰', 'DL']",
            "embedding": [0.15, 0.25, 0.35]
        }
    }]]

    # è®¾ç½® mock è¿”å›å€¼
    mock_client.search.return_value = mock_hits
    mock_client.upsert.return_value = True

    # æ¨¡æ‹Ÿ get_embedding å‡½æ•°
    with patch("SemanticTagger.get_embedding", return_value=[0.12, 0.22, 0.32]):
        # æ¨¡æ‹Ÿ tag_contents å‡½æ•°
        with patch("SemanticTagger.tag_contents", return_value=[
            ["AI", "æ·±åº¦å­¦ä¹ "],               # å½“å‰æ–‡æ¡£çš„æ ‡ç­¾
            ["AI", "é‡‘èç§‘æŠ€"],               # ç›¸ä¼¼æ–‡æ¡£1çš„æ–°æ ‡ç­¾
            ["æ·±åº¦å­¦ä¹ ", "å›¾åƒå¤„ç†"]           # ç›¸ä¼¼æ–‡æ¡£2çš„æ–°æ ‡ç­¾
        ]):
            updated_records = tagger.InsertTimeTagging(
                records=[test_record],
                batch_size=2,
                sample_scale=2,
                update_reference_records=False
            )

            # éªŒè¯ç»“æœ
            print("æ›´æ–°åçš„è®°å½•ï¼š")
            for r in updated_records:
                print(json.dumps(r, ensure_ascii=False, indent=2))

            assert len(updated_records) == 2, "åº”æ›´æ–°ä¸¤æ¡å‚è€ƒè®°å½•"
            print("âœ… InsertTimeTagging æµ‹è¯•é€šè¿‡")


def test_RetrievalTimeTagging():
    """
    æµ‹è¯• RetrievalTimeTagging æ–¹æ³•ï¼š
    - èƒ½å¦ä¸ºç°æœ‰è®°å½•æ‰¹é‡è¡¥å……æ–°æ ‡ç­¾ï¼›
    - èƒ½å¦ä¿ç•™æ—§æ ‡ç­¾å¹¶åˆå¹¶å»é‡ã€‚
    """
    print("\n=== å¼€å§‹æµ‹è¯• RetrievalTimeTagging ===")

    # åˆå§‹åŒ– SemanticTagger å®ä¾‹
    tagger = SemanticTagger(batch_size=2)

    # æ„é€ æµ‹è¯•è®°å½•ï¼ˆå«å·²æœ‰æ ‡ç­¾ï¼‰
    test_records = [
        {"id": 100, "content": "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯AIçš„å…³é”®é¢†åŸŸ", "tags": ["NLP", "è¯­è¨€æ¨¡å‹"]},
        {"id": 101, "content": "å¼ºåŒ–å­¦ä¹ æ¨åŠ¨æœºå™¨äººæ™ºèƒ½åŒ–å‘å±•", "tags": ["æœºå™¨äººå­¦"]},
        {"id": 102, "content": "æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å›¾åƒè¯†åˆ«ä¸­è¡¨ç°ä¼˜å¼‚", "tags": ["å›¾åƒè¯†åˆ«", "CNN"]},
        {"id": 103, "content": "å¤§æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›", "tags": []},
        {"id": 104, "content": "æ¨èç³»ç»Ÿä¾èµ–ç”¨æˆ·è¡Œä¸ºæ•°æ®åˆ†æ", "tags": ["æ¨èç³»ç»Ÿ", "ååŒè¿‡æ»¤"]},
        {"id": 105, "content": "å›¾ç¥ç»ç½‘ç»œç”¨äºç¤¾äº¤ç½‘ç»œåˆ†æ", "tags": ["GNN", "ç¤¾äº¤ç½‘ç»œ"]},
        {"id": 106, "content": "è¿ç§»å­¦ä¹ è®©å°æ•°æ®ä¹Ÿèƒ½è®­ç»ƒå‡ºå¥½æ¨¡å‹", "tags": ["è¿ç§»å­¦ä¹ "]},
        {"id": 107, "content": "Transformeræ¶æ„å¼•é¢†åºåˆ—å»ºæ¨¡æ–°æ—¶ä»£", "tags": ["Transformer", "æ³¨æ„åŠ›æœºåˆ¶"]},
        {"id": 108, "content": "è¾¹ç¼˜è®¡ç®—ç»“åˆAIå®ç°ä½å»¶è¿Ÿæ¨ç†", "tags": ["è¾¹ç¼˜è®¡ç®—", "AIéƒ¨ç½²"]},
        {"id": 109, "content": "è”é‚¦å­¦ä¹ ä¿æŠ¤ç”¨æˆ·éšç§çš„åŒæ—¶æå‡æ¨¡å‹æ•ˆæœ", "tags": ["è”é‚¦å­¦ä¹ ", "éšç§ä¿æŠ¤"]}
    ]

    # æ¨¡æ‹Ÿ tag_contents è¿”å›æ–°æ ‡ç­¾
    with patch("SemanticTagger.tag_contents", return_value=[
        ["NLP", "è¯­ä¹‰åˆ†æ"],
        ["å¼ºåŒ–å­¦ä¹ ", "æ™ºèƒ½æ§åˆ¶"]
    ]):
        updated_records = tagger.RetrievalTimeTagging(test_records, batch_size=2)

        print("æ›´æ–°åçš„è®°å½•ï¼š")
        for r in updated_records:
            print(json.dumps(r, ensure_ascii=False, indent=2))

        print("âœ… RetrievalTimeTagging æµ‹è¯•é€šè¿‡")


if __name__ == "__main__":
    print("ğŸŸ¢ å¼€å§‹è¿è¡Œè¯­ä¹‰æ ‡ç­¾å™¨æµ‹è¯•...\n")

    # try:
    #     test_InsertTimeTagging()
    # except AssertionError as e:
    #     print(f"ğŸ”´ InsertTimeTagging æµ‹è¯•å¤±è´¥: {e}")

    try:
        test_RetrievalTimeTagging()
    except AssertionError as e:
        print(f"ğŸ”´ RetrievalTimeTagging æµ‹è¯•å¤±è´¥: {e}")

    print("\nğŸ“Š æµ‹è¯•å®Œæˆ")